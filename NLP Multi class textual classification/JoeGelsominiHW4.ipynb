{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name: Joe Gelsomini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This homework consists of two parts: the first one is a theoritical problem and the second one focuses on the classification task. Text classification is a well-addressed task in NLP and it aims to assign a label to a given document (i.e. training example). You are  going to run and then compare a conventional machine learning model with a deep learning model that uses a recurrent neural network. In this part of the homework, you are expected to be familiar with conventional machine learning models. Note that this homework relies on `sklearn` library that is by far the most popular library in Python to run conventional ML models. If you go to its' documentation, you will find a lot of useful information on how to run and evaluate your models, how to extract useful features and transform the dataset; the documentation has a lot of examples that you can refer to.\n",
    "\n",
    "In the theoretical problem, you will be comparing different language models applied to a span of text.\n",
    "\n",
    "__You will need__ to have the following libraries installed (please make sure you followed the installation instructions in the class repository to install them):\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "- [NumPy](http://www.numpy.org/)\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) (`pip install sklearn`)\n",
    "\n",
    "\n",
    "__You will learn:__\n",
    "- How a language model works\n",
    "- How to represent input text using the TF-IDF algorithm\n",
    "- How to run conventional machine learning models for text classification\n",
    "- How to train a Recurrent Neural Network\n",
    "\n",
    "Once you complete this assignment, submit it as:\n",
    "\n",
    "`submit arum hw4 <name_of_your_notebook> <additional_files>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure that execution of this cell doesn't return any errors. If it does, go the class repository and follow the environment setup instructions\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, sent_tokenize #added sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Language model\n",
    "In this problem, we will be computing probabilities assigned to a particular word sequence by different [language models](https://en.wikipedia.org/wiki/Language_model), given the training data below. You are not required to submit any code for this problem.\n",
    "\n",
    "__Instructions__:\n",
    "- paste your answers to the following questions into the notebook.\n",
    "- if you do write your own code to solve the problem, please include it, otherwise please sumbit additional files showing your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Harold Jones , Lincoln ’s chief retail banking officer , said there has n’t yet been “ a discernible response , ” although the ad included a coupon that could arrive later in the week .\n",
    "In a move which could pose a new competitive challenge to Time Warner Inc. ’s powerful Home Box Office , cable giant Tele-Communications Inc. agreed to buy half of Showtime Networks Inc.\n",
    "from Viacom Inc. for 225 million. The oil and auto industries , united in their dislike of President Bush ’s proposal for cars that run on alternative fuels , announced a joint research program which could turn up a cleaner-burning gasoline. The derivative mortgage-backed market revived after the news about two more issues that could be priced today. Japanese air-conditioner maker Daikin Industries Ltd. was fined two million yen for exporting to the Soviet Union chemicals in a solution that could be used in missile-guidance systems. We at Ledbury Research believe there are knowledge gaps on both sides of the table. The above agreement must be signed by both parties and acknowledged by all sides. Tax returns will be accepted by the IRS electronically.\n",
    "Friends long have found inspiration in George Fox ’s words, ”We seek a world free of war and thethreat of war”.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Compute the probability assigned to the following string by a bigram language model applied __without smoothing__:\n",
    "`We seek a solution that could be accepted by both sides.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW2 Function from sample solution courtesy of graders :)\n",
    "# Slightly modified to convert all tokens to lower\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (str):\n",
    "    Returns: a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = [t.lower() for t in word_tokenize(data) if t not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P of: we given <s> 0.1111111111111111\n",
      "P of: seek given we 0.5\n",
      "P of: a given seek 1.0\n",
      "P of: solution given a 0.125\n",
      "P of: that given solution 1.0\n",
      "P of: could given that 0.75\n",
      "P of: be given could 0.4\n",
      "P of: accepted given be 0.25\n",
      "P of: by given accepted 1.0\n",
      "P of: both given by 0.3333333333333333\n",
      "P of: sides given both 0.5\n",
      "P of: </s> given sides 0.5\n",
      "\n",
      "Probability of sentence: 4.340277777777777e-05\n"
     ]
    }
   ],
   "source": [
    "#create word doc\n",
    "doc = \"Harold Jones , Lincoln ’s chief retail banking officer , said there has n’t yet been “ a discernible response , ” although the ad included a coupon that could arrive later in the week. In a move which could pose a new competitive challenge to Time Warner Inc. ’s powerful Home Box Office , cable giant Tele-Communications Inc. agreed to buy half of Showtime Networks Inc. from Viacom Inc. for 225 million. The oil and auto industries , united in their dislike of President Bush ’s proposal for cars that run on alternative fuels , announced a joint research program which could turn up a cleaner-burning gasoline. The derivative mortgage-backed market revived after the news about two more issues that could be priced today. Japanese air-conditioner maker Daikin Industries Ltd. was fined two million yen for exporting to the Soviet Union chemicals in a solution that could be used in missile-guidance systems. We at Ledbury Research believe there are knowledge gaps on both sides of the table. The above agreement must be signed by both parties and acknowledged by all sides. Tax returns will be accepted by the IRS electronically. Friends long have found inspiration in George Fox ’s words, ”We seek a world free of war and the threat of war”.\"\n",
    "sent = \"We seek a solution that could be accepted by both sides\"\n",
    "\n",
    "#seperate sentences\n",
    "sentences = sent_tokenize(doc)\n",
    "marked_sents = []\n",
    "\n",
    "#add a sentence start \"<s>\" and stop \"</s>\" token to each sentence\n",
    "######Doing this gives much smaller probabilities, because of quotes in corpus#####\n",
    "for sentence in sentences:\n",
    "    tokens = preprocess(sentence)\n",
    "    tokens.insert(0,'<s>')#can comment out\n",
    "    tokens.append(\"</s>\")#can comment out\n",
    "    marked_sents.append(tokens)\n",
    "\n",
    "\n",
    "#counts of unique tokens in doc\n",
    "counts = Counter()\n",
    "bigrams = []\n",
    "\n",
    "for sentence in marked_sents:\n",
    "    for i in range(len(sentence)):\n",
    "        counts[sentence[i]] += 1\n",
    "        if i+1 < len(sentence):\n",
    "            bigrams.append((sentence[i], sentence[i+1]))\n",
    "\n",
    "#make bigrams for sentence we want to compute probability of\n",
    "sent = preprocess(sent)\n",
    "sent_bigrams = []\n",
    "\n",
    "####You can comment out the start and end markers to get smaller probabilities\n",
    "sent.insert(0,\"<s>\")#can be removed\n",
    "sent.append(\"</s>\")#can be removed\n",
    "for i in range(len(sent)-1):\n",
    "    if i+ 1 < len(sent):\n",
    "        sent_bigrams.append((sent[i],sent[i+1]))\n",
    "        \n",
    "#now have everything we need to cmpute the probability of the sentence, with and without absolute discounting\n",
    "probability = 1\n",
    "probability_abs = 1\n",
    "for bg in sent_bigrams:\n",
    "    print(\"P of:\", bg[1],\"given\",bg[0], (bigrams.count(bg)/(counts[bg[0]])))\n",
    "    probability *= (bigrams.count(bg)/(counts[bg[0]]))\n",
    "    #probability_abs *= (bigrams.count(bg) -0.03 )/ (counts[bg[0]])\n",
    "    \n",
    "print(\"\\nProbability of sentence:\",probability)\n",
    "\n",
    "\n",
    "#print(\"\\nBigrams generated:\\n\",bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Compute the probability assigned to the above string by a bigram language model with __absolute discounting (δ =.03)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P absolute of: we given <s> 0.10777777777777778\n",
      "P absolute of: seek given we 0.485\n",
      "P absolute of: a given seek 0.97\n",
      "P absolute of: solution given a 0.12125\n",
      "P absolute of: that given solution 0.97\n",
      "P absolute of: could given that 0.7425\n",
      "P absolute of: be given could 0.394\n",
      "P absolute of: accepted given be 0.2425\n",
      "P absolute of: by given accepted 0.97\n",
      "P absolute of: both given by 0.3233333333333333\n",
      "P absolute of: sides given both 0.485\n",
      "P absolute of: </s> given sides 0.485\n",
      "\n",
      "Probability with absolute discounting: 3.1210900058228693e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "probability_abs = 1\n",
    "for bg in sent_bigrams:\n",
    "    print(\"P absolute of:\", bg[1], \"given\", bg[0], (bigrams.count(bg) - 0.03) / (counts[bg[0]]) )\n",
    "    probability_abs *= (bigrams.count(bg) - 0.03 )/ (counts[bg[0]])\n",
    "          \n",
    "print(\"\\nProbability with absolute discounting:\",probability_abs,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Category Dataset\n",
    "This dataset is a collection of headlines and the corresponding categories obtained from [HuffPost](https://www.huffingtonpost.com/). This dataset is publicly available on the Kaggle Platform: [https://www.kaggle.com/rmisra/news-category-dataset/](https://www.kaggle.com/rmisra/news-category-dataset/) In total, it contains 41 categories with a different number of samples in each category. For the sake of simplicity, in this homework we are going to use only 8 categories, listed below. Our classification task will be to predict the category of a given headline.\n",
    "\n",
    "__Notes__:\n",
    "Make sure the dataset is downloaded and unpacked to the same directory as this notebook.\n",
    "\n",
    "__Instructions__:\n",
    "- Run the cells below to load the dataset from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading the dataset from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_categories = {\n",
    "    'BUSINESS',\n",
    "    'COMEDY',\n",
    "    'FOOD & DRINK',\n",
    "    'HEALTHY LIVING',\n",
    "    'PARENTING',\n",
    "    'QUEER VOICES',\n",
    "    'STYLE & BEAUTY',\n",
    "    'TRAVEL',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, categories=None):\n",
    "    samples = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            if categories is None or sample['category'] in categories:\n",
    "                samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('News_Category_Dataset_v2.json', needed_categories)\n",
    "\n",
    "# For convenience, let's store the headlines and the corresponding categories in separate lists\n",
    "data_headlines = [x['headline'] for x in data]\n",
    "data_labels = [x['category'] for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a random example to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:\n",
      " STYLE & BEAUTY\n",
      "Headline:\n",
      " 7 Hairstyles To Cross Off Your Bucket List\n"
     ]
    }
   ],
   "source": [
    "random_sample_idx = random.randrange(len(data))\n",
    "print('Category:\\n', data_labels[random_sample_idx])\n",
    "print('Headline:\\n', data_headlines[random_sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Now let's do the dataset analysis.\n",
    "\n",
    "__Instructions__:\n",
    "Complete the code in the cells below to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 How many examples are there in the resulting dataset (i.e. in the subset of the original dataset)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58559 examples\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "num_examples = len(data_headlines)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('There are {} examples'.format(num_examples))\n",
    "#print(\"\\n\",data_headlines[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Split the dataset into train and test subsets in the 70/30 ratio.\n",
    "\n",
    "__Notes__:\n",
    "- Use the `train_test_split` function from the `sklearn` library and __set the `random_state` argument to 10__. This is important for getting consistent results over multiple runs (and for grading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_headlines, data_labels, test_size = .3, random_state = 10)\n",
    "data_train = X_train\n",
    "labels_train = y_train\n",
    "data_test = X_test\n",
    "labels_test = y_test\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 How many samples are there in the training split, and how many samples are there in the test split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40991 training and 17568 test examples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "num_train_examples = len(X_train)\n",
    "num_test_examples = len(X_test)\n",
    "#print(num_train_examples + num_test_examples, \"total examples.\")\n",
    "### YOUR CODE ABOVE ###\n",
    "print(\"There are {} training and {} test examples in the dataset.\".format(num_train_examples, num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Plot the histogram of the class counts for both subsets.\n",
    "\n",
    "__Notes__:\n",
    "- In general, it is always a good idea to check the data distributions for your dataset splits.\n",
    "- Use the `Counter` class from the `collections` library to count examples belonging to each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "counter_train = Counter()\n",
    "counter_test = Counter()\n",
    "\n",
    "for label in y_train:\n",
    "    counter_train[label] +=1\n",
    "for label in y_test:\n",
    "    counter_test[label] +=1\n",
    "    \n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "names = [c for c, n in counter_train.most_common()]\n",
    "index = np.arange(len(names))\n",
    "\n",
    "values_train = [counter_train[c] for c in names]\n",
    "values_test = [counter_test[c] for c in names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.bar(index-0.2, values_train, label=\"train\", width=0.4)\n",
    "ax.bar(index+0.2, values_test, label=\"test\", width=0.4)\n",
    "\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(names, rotation=30)\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 What is the most represented category in the dataset? What is the least represented category in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAVEL is the most frequent label in the data, with 9887 instances.\n",
      "COMEDY is the most infrequent label, with 5175 instances.\n"
     ]
    }
   ],
   "source": [
    "counter_all = Counter()\n",
    "for label in counter_train:\n",
    "    counter_all[label] = counter_train[label] + counter_test[label]\n",
    "print(counter_all.most_common()[0][0],\"is the most frequent label in the data, with\", \n",
    "      counter_all.most_common()[0][1], \"instances.\")\n",
    "print(counter_all.most_common()[len(counter_all)-1][0], \"is the most infrequent label, with\",\n",
    "      counter_all.most_common()[len(counter_all)-1][1], \"instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF\n",
    "\n",
    "In previous homeworks you familiarized yourself with the simplest method for representing a given document in a corpus - through constructing a vector of raw word counts for every training example. While this method works for a number of tasks and datasets, a better alternative way of handling text is to use so-called TF-IDF representations.\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term-frequency-inverse-document-frequency) is a common technique in NLP that is based on selecting the most important words for a given training example, where the importance of every word is defined by (a) how frequently it is used in the entire corpus, and (b) how frequently it is used in the given example. More specifically, if a given word is relatively rare for the entire collection of documents but it occurs many times within a single document, then this word is important for \"defining\" what this document is about. In the current step you will convert a set of texts to their TF-IDF representations. You will use the obtained representations as features for training text classification models later on. Sklearn library provides a `TfidfVectorizer` class that you are going to use (please refer to the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code in `transform_to_tfidf` that takes a collection of documents (train and test examples) and returns a list of corresponding TF-IDF vectors.\n",
    "- In the cells below, run the function with the correct arguments and complete the coded where needed to answer the questions\n",
    "\n",
    "__Notes__:\n",
    "- There is no need for you to tokenize the input examples since `TfidfVectorizer` does it for you\n",
    "- `TfidfVectorizer` returns a sparse array; you might want to convert it to the dense representation using `toarray()`\n",
    "- Make sure to understand the difference between the following methods: `fit()`, `fit_transform()` and `transform()`. These methods are very common for sklearn pipeline and used not only for dataset processing, but also for model running and evaluating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tfidf(train_corpus, test_corpus, max_df=1.0, min_df=1, stop_words=None):\n",
    "    \"\"\"\n",
    "    Transform train and test documents to their TF-IDF representations and return the features.\n",
    "    Args: \n",
    "        train_corpus (list) : a list of str documents\n",
    "        test_corpus (list) : a list of str documents\n",
    "    Returns:\n",
    "        a tuple of the transformed train matrix, the transformed test matrix and the list of words \n",
    "        used by the TfidfVectorizer as features\n",
    "    \n",
    "    \"\"\"\n",
    "    ### YOUR CODE BELOW ###\n",
    "    #print(type(train_corpus))\n",
    "\n",
    "    #corpus = train_corpus + test_corpus\n",
    "    vec = TfidfVectorizer(max_df = max_df, min_df = min_df, stop_words = stop_words)\n",
    "    vec.fit(train_corpus)\n",
    "    X_train = vec.transform(train_corpus)\n",
    "    X_test = vec.transform(test_corpus)\n",
    "    feature_names = vec.get_feature_names() \n",
    "    ### YOUR CODE ABOVE ###\n",
    "    return X_train.toarray(), X_test.toarray(), feature_names\n",
    "    #return X_train, X_test, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 If you call the `transform_to_tfidf` function with default parameters, what would be the dimensionality of your dataset examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensionality of a single example is 27080 features\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "tfidf = transform_to_tfidf(X_train, X_test)\n",
    "dimensionality = len(tfidf[2])\n",
    "### YOUR CODE ABOVE ###\n",
    "print(\"The dimensionality of a single example is {} features\".format(dimensionality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What are the top-3 most important words (i.e. the words with the highest TF-IDF scores) for the 43rd train example? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43rd label:\t BUSINESS\n",
      "43rd headline:\t An Anonymous Rich Person Is Hiding Money All Around San Francisco \n",
      "\n",
      "The top-3 most important words are:\n",
      "\n",
      "Words\t\t\t\t ['rich', 'hiding', 'anonymous']\n",
      "TF-IDF scores\t\t\t [0.3408296  0.38714305 0.41126347]\n",
      "indices into featurenames\t [20362 11321  1256]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "#find indices of the 3 max tfidf scores in 43rd train example\n",
    "top3 = np.argpartition(tfidf[0][42],-3)[-3:]\n",
    "top_words =[]\n",
    "\n",
    "for i in top3:#grab words from the indices\n",
    "    top_words.append(tfidf[2][i])\n",
    "    #print(tfidf[0][42][i])\n",
    "\n",
    "print(\"43rd label:\\t\",y_train[42])\n",
    "print(\"43rd headline:\\t\",X_train[42],\"\\n\")\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print(\"The top-3 most important words are:\\n\")\n",
    "\n",
    "print(\"Words\\t\\t\\t\\t\",top_words)\n",
    "print(\"TF-IDF scores\\t\\t\\t\", tfidf[0][42][top3] )\n",
    "print(\"indices into featurenames\\t\",top3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Now, use the `transform_to_tfidf` again but filter words that have a document frequency strictly higher than 0.95, words that appear strictly less than in 10 documents, and english stop-words. What is the dimensionality of your training samples now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensionality of a single example is 4357 features\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "from nltk.corpus import stopwords\n",
    "############## PLEASE READ ############################\n",
    "#I originally did not have stop words installed, I ran these 2 lines below to fix that\n",
    "#nltk documentation states that the string \"english\" can be passed instead, but is supposedly less relaible\n",
    "#then using the english set from nltks stopwords\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "tfidf_filtered = transform_to_tfidf(X_train, X_test, max_df = 0.95, min_df =10, stop_words = stopwords)\n",
    "dimensionality = len(tfidf_filtered[2])\n",
    "\n",
    "### YOUR CODE ABOVE ###\n",
    "print(\"The dimensionality of a single example is {} features\".format(dimensionality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What are the top-3 most important words for the 43rd training example now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43rd label:\t BUSINESS\n",
      "43rd headline:\t An Anonymous Rich Person Is Hiding Money All Around San Francisco \n",
      "\n",
      "The top-3 most important words are:\n",
      "\n",
      "Words\t\t\t\t ['francisco', 'rich', 'hiding']\n",
      "TF-IDF scores\t\t\t [0.38490088 0.40255944 0.45726102]\n",
      "indices into featurenames\t [1578 3242 1833]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "#find indices of the 3 max tfidf scores in 43rd train example\n",
    "top3 = np.argpartition(tfidf_filtered[0][42],-3)[-3:]\n",
    "top_words =[]\n",
    "\n",
    "for i in top3:#grab words from the indices\n",
    "    top_words.append(tfidf_filtered[2][i])\n",
    "    #print(tfidf[0][42][i])\n",
    "\n",
    "print(\"43rd label:\\t\",y_train[42])\n",
    "print(\"43rd headline:\\t\",X_train[42],\"\\n\")\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print(\"The top-3 most important words are:\\n\")\n",
    "\n",
    "print(\"Words\\t\\t\\t\\t\",top_words)\n",
    "print(\"TF-IDF scores\\t\\t\\t\", tfidf_filtered[0][42][top3] )\n",
    "print(\"indices into featurenames\\t\",top3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM\n",
    "\n",
    "In this part of the homework you are going to train a [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine) classifier as one of examples of classic Machine Learning models. Recall that, unlike most of classification models, support vector machines (SVMs) are not probabilistic: instead, they are trying to separate the classes through maximization of the margin. \n",
    "\n",
    "Sklearn provides a number of SVM-based models; you are going to use a linear SVM classifier (`LinearSVC` class).\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code in the cells below to answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fit the linear SVC model with default parameters and make predictions on the test data. What accuracy does your model achieve on the test set?\n",
    "\n",
    "__Notes__:\n",
    "Set the `random_state` argument to 10 to ensure the consistency of your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The achieved test accuracy is 0.792\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "lsvc = LinearSVC(random_state = 10)\n",
    "x_train = tfidf[0] #train samples from transform_to_tfidf section 3.1\n",
    "x_test = tfidf[1] #test samples\n",
    "lsvc.fit(x_train,y_train) #y_train defined above, labels for each training headline\n",
    "\n",
    "accuracy = lsvc.score(x_test, y_test)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print(\"The achieved test accuracy is {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that running a default model without any fine-tuning can give you significantly lower scores than what is potentially achievable by a given model class and for a given task. Instead, you should always try running a number of models with different hyperparameter configurations and pick the configuration that achieves the best score on the held-out set (i.e. validation set). If your dataset is small enough that you can't afford holding a subset of it for hyperparameter tuning, you should perform [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). Sklearn provides a technique for fine-tuning a given model which is called [grid search](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) (`GridSearchCV` class). The idea is to introduce a grid of hyperparameters' values and run the model for every single combination of hyperparameters. We will be optimizing the `C` hyperparameter of `LinearSVC` that controls the size of the margin.\n",
    "\n",
    "\n",
    "__Instructions__:\n",
    "- Using `GridSearchCV` fine-tune your classifier for a set of values of `C` that is given below\n",
    "- Make predictions for the test set using the best of the models and report the accuracy\n",
    "- Use 4-folds cross-validation (`cv`=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What is the test accuracy achieved by the best of the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The achieved test accuracy is 0.798\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"C\": [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "### YOUR CODE BELOW ###\n",
    "GSCV = GridSearchCV(LinearSVC(random_state = 10), params, cv = 4)\n",
    "GSCV.fit(x_train,y_train)\n",
    "accuracy = GSCV.score(x_test, y_test)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print(\"The achieved test accuracy is {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. What is the most optimal value for `C`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value for C is 0.3.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "best = GSCV.best_params_\n",
    "print('Best value for C is {}.'.format(best['C']))\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after we trained a traditional machine learning algorithm, let's solve this task using neural networks. As in the homework 3, we will need the following classes: a vocabulary class, a dataset class, and a model class. We will re-use the vocabulary class from the previous howmework, and implement the dataset and the model classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Vocabulary class\n",
    "Run the cell below - it's a reference vocabulary implementation that you saw in the previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        #print(set(self.special_tokens))\n",
    "        \n",
    "        #INSERTED THESE CHANGES MYSELF\n",
    "        for token in self.special_tokens:\n",
    "            if token in to_remove:\n",
    "                to_remove.remove(token)\n",
    "        #THE ORIGINAL CODE IN PRUNE WAS REMOVING <PAD> FROM THE VOCAB\n",
    "        #to_remove ^= set(self.special_tokens)\n",
    "        #print('<PAD>' in to_remove,self.w2cnt['<PAD>'])\n",
    "\n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "\n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "        \n",
    "        #print(\"Pad in w2cnt\", '<PAD>' in self.w2cnt)\n",
    "        #print(\"Pad in w2idx\", '<PAD>' in self.w2idx)\n",
    "        #print(\"Pad in idx2w\", self.idx2w[0] is '<PAD>')\n",
    "        #print(self.w2idx['<PAD>'])\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dataset class\n",
    "The class we are going to implement will be a universal class that can be used in any text classification task. This class accepts a list of texts and the corresponding labels in the conыtructor and performs an inшtial pre-processing of a text sample. Namely, it tokenizes the text and cuts it up to the specified `max_len`. After that, it creates the vocabulary objects that are going to be used to convert text samples and their labels into a numeric representation. The same vocabulary class can be used to convert labels into indices as well. Note, however, that we want to create the vocabulary __only over the training set__, so the dataset object for the test split will use the vocabulary objects from the dataset object __for the training split__. This is why the `TextClassificationDataset` class accepts the `vocab` and `labels_vocab` arguments. If those arguments are not provided, it should create new vocabularies and populate them with the corresponding tokens. Otherwise, it should just use the provided arguments.\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code in `TextClassificationDataset` to answer the questions below.\n",
    "- Make sure you crop dataset examples with lengths that exceed the `max_len` parameter.\n",
    "- After completing the code in the dataset class, run the cells below to initialize your dataset.\n",
    "\n",
    "__Notes__:\n",
    "- The special `<PAD>` token is used to pad sequences to the same length.\n",
    "- The special `<UNK>` token is used for tokens in the test set that do not appear in the train set (make sure to use it in your `__getitem__()` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None, labels_vocab=None, max_len=40, lowercase=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): texts of the dataset examples\n",
    "            labels (list of str): the correponding labels of the dataset examples\n",
    "            vocab (MyVocabulary, optional): vocabular to convert text to indices. If not provided, will be created based on the texts\n",
    "            labels_vocab (MyVocabulary, optional): vocabular to convert labels to indices. If not provided, will be created based on the labels\n",
    "            max_len (int): maximum length of the text. Texts shorter than max_len will be cut at the end\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "\n",
    "        self.texts = [self._preprocess(t, max_len=max_len, lowercase=lowercase) for t in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "        if vocab is None:\n",
    "            vocab = MyVocabulary(['<PAD>', '<UNK>'])\n",
    "            for text in self.texts:\n",
    "                vocab.add_tokens(text)\n",
    "            \n",
    "        if labels_vocab is None:\n",
    "            labels_vocab = MyVocabulary()\n",
    "            labels_vocab.add_tokens(labels)\n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.labels_vocab = labels_vocab\n",
    "        \n",
    "    def _preprocess(self, text, max_len=None, lowercase=True):\n",
    "        \"\"\"\n",
    "        Preprocess a give dataset example\n",
    "        Args:\n",
    "            text (str): given dataset example\n",
    "            max_len (int, optional): maximum sequence length\n",
    "            lowercase (bool, optional): a fag specifying whether or not the input text should be lowercased\n",
    "        \n",
    "        Returns:\n",
    "            a list of tokens for a given text span\n",
    "        \"\"\" \n",
    "        ### YOUR CODE BELOW ###\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        # tokenize the input text\n",
    "       \n",
    "        tokens = []\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # cut the list of tokens to `max_len` if needed \n",
    "        if len(tokens) > max_len:\n",
    "            del tokens[max_len:]\n",
    "        tokens = self._pad(tokens)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "    \n",
    "    def _pad(self, tokens):\n",
    "        \"\"\"\n",
    "        Pad tokens to self.max_len\n",
    "        Args:\n",
    "            tokens (list): a list of str tokens for a given example\n",
    "            \n",
    "        Returns:\n",
    "            list: a padded list of str tokens for a given example\n",
    "        \"\"\"\n",
    "        # pad the list of tokens to be exactly of the `max_len` size\n",
    "        ### YOUR CODE BELOW ###\n",
    "        if len(tokens) < self.max_len:\n",
    "            for i in range(len(tokens), self.max_len):\n",
    "                tokens.append(\"<PAD>\") \n",
    "        ### YOUR CODE ABOVE ###\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given an index, return a formatted dataset example\n",
    "        \n",
    "        Args:\n",
    "            idx (int): dataset index\n",
    "            \n",
    "        Returns:\n",
    "            tuple: a tuple of token_ids based on the vocabulary mapping  and a corresponding label\n",
    "        \"\"\"\n",
    "        ### YOUR CODE BELOW ###\n",
    "        tokens = []\n",
    "        words = self.texts[idx]\n",
    "        for word in words:\n",
    "            if word not in self.vocab.w2cnt:\n",
    "                #print(word)\n",
    "                tokens.append(self.vocab.w2idx['<UNK>'])\n",
    "            elif word != '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[word])\n",
    "            elif word == '<PAD>':\n",
    "                tokens.append(self.vocab.w2idx[\"<PAD>\"])\n",
    "                #print(\"found pad\")\n",
    "        label = self.labels[idx]\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        return  np.asarray(tokens), np.asarray(self.labels_vocab.w2idx[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextClassificationDataset(data_train, labels_train)\n",
    "dataset_test = TextClassificationDataset(data_test, labels_test, vocab=dataset_train.vocab, labels_vocab=dataset_train.labels_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.vocab.prune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 What is the size of the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15560 words in the training vocab.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "print(\"There are {} words in the training vocab.\".format(len(dataset_train.vocab.idx2w)))\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Print the first 10 tokens of the first example in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'urgencies', 'of', 'care', ':', 'here', ',', 'there', 'and', 'everywhere']\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "print(dataset_train.texts[0][0:10])\n",
    "#print(dataset_train.vocab.w2cnt[\"urgencies\"])\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Print the first 10 elements of the correponding item (the first item) in the dataset (that is, converted to indices sample after the `__getitem__` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  1  3  4  5  6  7  8  9 10]\n",
      "the <UNK> of care : here , there and everywhere \n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "x = dataset_train.__getitem__(0)\n",
    "print(x[0][0:10])\n",
    "#taking a look at the index mapping:\n",
    "mystring = \"\"\n",
    "for idx in x[0][0:10]:\n",
    "    mystring += str(dataset_train.vocab.idx2w[idx]) + \" \"\n",
    "print(mystring)\n",
    "### YOUR CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for a text classification task consists of three main parts: \n",
    " - an embedding layer, that converts indices of tokens into dense vectors \n",
    " - an RNN layer, that processes every token in a sequence in a sequential manner using the [GRU architecture](https://pytorch.org/docs/stable/nn.html#gru)\n",
    " - a projection layer, that projects a hidden state $z$ to a probability distribution over the classes\n",
    " \n",
    "The `forward` method just applies these layers in a sequential manner. One important note here is that the RNN layer returns hidden states at each time step (where a time step corresponds to a token in a sequence). To get a single vector representation of each example, we'll use max pooling over time steps.\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code in `TextClassificationModel`.\n",
    "\n",
    "__Notes__:\n",
    "- Use `torch.max()` to perform maxpooling.\n",
    "- Like in the previous homework, you don't need to apply an activation function at the output layer of the network - PyTorch takes care of it for you while calculating the loss.\n",
    "- Make sure your training loss is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, nb_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        ### YOUR CODE BELOW ###\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.GRU = torch.nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "        self.projection = torch.nn.Linear(hidden_size, nb_classes)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        embeds = self.embeddings(inputs)\n",
    "        outputs, garbage = self.GRU(embeds)\n",
    "        outputs = torch.max(outputs, dim = 1)[0]\n",
    "        #print(outputs)\n",
    "        logits = self.projection(outputs)\n",
    "        ### YOUR CODE ABOVE ###        \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training loop\n",
    "\n",
    "Carefully read through the cells below to understand the parameters passed to the network. Run the training loop cell to check if your model is training.\n",
    "\n",
    "__Warning__: Training can take a while. __Make sure you use GPUs__ and have tested your implementation on a smaller subset of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER #\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=64)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle=False, batch_size=64)\n",
    "\n",
    "# MODEL INITIALIZATION #\n",
    "model = TextClassificationModel(64, len(dataset_train.vocab), 128, len(needed_categories))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# OPTIMIZER #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# LOSS-FUNCTION #\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 1.5472997638067105\n",
      "Epoch 1, loss 0.9918294032910685\n",
      "Epoch 2, loss 0.7595846181540705\n",
      "Epoch 3, loss 0.6078404056579572\n",
      "Epoch 4, loss 0.48899827439587873\n",
      "Epoch 5, loss 0.39152779817209227\n",
      "Epoch 6, loss 0.3086008203104022\n",
      "Epoch 7, loss 0.2428632832354577\n",
      "Epoch 8, loss 0.187869868417054\n",
      "Epoch 9, loss 0.1432487843398371\n"
     ]
    }
   ],
   "source": [
    "# TRAINING #\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    epoch_losses = []\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        #print(y)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    losses.append(epoch_loss)\n",
    "    print('Epoch {}, loss {}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Prediction\n",
    "Ceate a function for passing the samples through the model to get predictions on new data. This function is conceptually similar to the traning loop and should return a tuple of `(y_true, y_pred)` numpy arrays.\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code in the `predict` function.\n",
    "- Recall that for making a prediction you need to take the class with the highest predicted probability. You can use `torch.argmax()` here.\n",
    "- Append your true and predicted labels to two separate lists/arrays. You will ise them later to report the accuracy.\n",
    "- Run the cell below to make predictions on the train and on the test set.\n",
    "\n",
    "__Notes__:\n",
    "- Note that you don't need to iterate over epochs for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"\n",
    "    Predict probability distributions over classes on the test data\n",
    "    \n",
    "    Args:\n",
    "        model: your torch.nn.Module() model object\n",
    "        dataloader: test Dataloder() object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: np.array/list with true labels and np.array/list with predicted labels\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    ### YOUR CODE BELOW ###\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        outputs = model(x)\n",
    "        y_pred.append(torch.argmax(outputs.data, 1))\n",
    "        y_true.append(y)\n",
    "        \n",
    "    ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)    \n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train, y_pred_train = predict(model, dataloader_train)\n",
    "y_true_test, y_pred_test = predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Report the accuracies on the trainig set and on the test set\n",
    "\n",
    "__Instructions__:\n",
    "- Complete the code below to report the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train 0.9745553902076065\n",
      "Accuracy test 0.7363957194899818\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE BELOW ###\n",
    "acc_train = accuracy_score(y_true_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
    "### YOUR CODE ABOVE ###\n",
    "\n",
    "print('Accuracy train', acc_train)\n",
    "print('Accuracy test', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
